# Cluster targeting (master still passed by script because it depends on your current kube-context)
spark.kubernetes.namespace               spark
spark.kubernetes.authenticate.driver.serviceAccountName  spark-sa

# Image (built & loaded by your dev flow)
spark.kubernetes.container.image         spark-processor:dev

# Pod templates (the submitter will mount this infra dir and point Spark here)
spark.kubernetes.driver.podTemplateFile  /mnt/infra/20-driver-pod-template.yml
spark.kubernetes.executor.podTemplateFile /mnt/infra/21-executor-pod-template.yml

# Resources â€“ purely config, tune as needed
spark.driver.memory                      1g
spark.executor.instances                 1
spark.executor.cores                     1
spark.executor.memory                    2g

# Streaming quality-of-life
spark.streaming.stopGracefullyOnShutdown true
# (Enable later if you want idle scale-to-zero)
# spark.dynamicAllocation.enabled          true
# spark.dynamicAllocation.minExecutors     0
# spark.dynamicAllocation.maxExecutors     4
spark.sql.catalog.clickhouse                com.clickhouse.spark.ClickHouseCatalog
spark.sql.catalog.clickhouse.host           clickhouse
spark.sql.catalog.clickhouse.protocol       http
spark.sql.catalog.clickhouse.http_port      8123
spark.sql.catalog.clickhouse.user           spark
spark.sql.catalog.clickhouse.password       sparkpass
spark.sql.catalog.clickhouse.database       quant
spark.clickhouse.write.format               json

# <- the two lines that avoid your crash
spark.clickhouse.ignoreUnsupportedTransform true
spark.clickhouse.write.repartitionByPartition false

